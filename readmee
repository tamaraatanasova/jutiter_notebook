Линеарна регресија

Регресијата е техника на надгледувано (supervised) машинско учење што се користи за предвидување на континуирани вредности. Наместо да класифицира во категории, регресијата се обидува да предвиди бројчани вредности.
Примери за регресија:
Предвидување на цената на стан според квадратурата и локацијата.
Проценка на температура за утрешниот ден.
Прогноза на профит на компанија според рекламен буџет.

Линеарната регресија е наједноставниот тип на регресија каде односот меѓу зависната променлива (target) и независните променливи (features) се претпоставува дека е линеарен.
y=β0​+​β1x+ϵ
y — Зависна променлива
Тоа е вредноста што сакаме да ја предвидиме.
Пример:
Ако предвидуваме цена на куќа → y е цената.


x — Независна променлива
Тоа се влезните податоци или фактори што влијаат на y.
Пример:
Големина на куќа (квадратура) → x.
β0​ — Пресечна точка (Intercept)
Ова е вредноста на y кога x=0
Пример:
Ако квадратурата е 0, ова е основната цена (на пример трошоци за земјиште или минимални фиксни трошоци).


β1​ — Наклон на правата (Slope)
Ова покажува колку y се менува кога x се зголемува за 1 единица.
Пример:
Ако β1=500, значи за секој нов 1 квадратен метар, цената расте за 500.
ϵ — Грешка (Error term)
Тоа е разликата помеѓу реалната вредност и предвидената вредност.
Бидејќи во реалниот свет не секогаш врската е совршено линеарна, оваа компонента ја покрива:
случајна варијација,
непредвидени фактори,
мали неточности.
Пример:
Куќа со иста квадратура може да биде поскапа ако има подобар поглед или локација, и тоа ϵ го објаснува.


Ако моделот ја најде равенката:
y=20+2x Ова значи:
β0=20→ Основна цена (можеби фиксни трошоци).
β1=2→ Секој квадратен метар ја зголемува цената за 2 илјади.

Ако станот има 75 m², пресметката е:
y=20+2(75)=20+150=170 (илјади)


Асоцијација

Association (Асоцијација) во Data Science најчесто се однесува на откривање скриени врски или обрасци меѓу различни објекти или настани во податочните сетови.
Типичен пример е асоцијативна анализа која се користи кај:
препораки на производи (како: "Купувачи кои купиле млеко, исто така купиле и леб"),
откривање правила на зависности во продажба (Association Rule Mining).
Најпознат алгоритам за ова е Apriori алгоритамот.
Основни поими:
Support: Колку често се појавуваат заедно артикли.
Confidence: Веројатност да се купи артикл Б ако е купен артикл А.
Lift: Колку е поголема шансата да се купи Б кога е купен А, споредено со случајно купување на Б.


Кластерирање
Да се идентификуваат природни групи во податоците, така што податоците во истиот кластер ќе бидат што е можно послични, а податоците од различни кластери што е можно поразлични.

Поделба на клиенти според нивното однесување при купување.
Групирање документи или статии според тематика.
Откривање аномалии (кластерите што се “изолирани”).
Групирање на гени во биоинформатика.
Класификација на слики, музика, корисници на мрежа, итн.

1. K-Means
Внес: број на кластери K.
Принцип: Алгоритмот ја доделува секоја точка до најблискиот центар (центроид), па ги ажурира центрите.
Пример: групирање клиенти според нивните потрошувачки навики.
✅ Брз, едноставен.
❌ Осетлив на избор на K и на аномалии.
2. Хиерархиско кластерирање
Не бара број на кластери однапред.
Градење на хиерархија на кластери (дендрограма)
Agglomerative: оддолу нагоре (спојување)
Divisive: одозгора надолу (раздвојување).
✅ Дава увид во структурата.
❌ Станува бавен за многу податоци.
3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
Не бара број на кластери.
Користи густина на податоци за да дефинира кластери.
✅ Добар за неправилни форми и отпорен на аномалии.
❌ Потребна е добра селекција на параметри.
Пред кластерирање:
Се стандардизираат податоците (нормализација).
Се елиминираат аномалии.
Понекогаш се применуваат техники за намалување на димензионалност (на пр. PCA).
Кластерирањето не гарантира уникатно точно решение - резултатите зависат од:
алгоритамот,
параметрите,
почетната состојба,
претходната обработка на податоците.

Feature Engineering

Feature = карактеристика, одлика, особина
Engineering = инженеринг, односно процес на дизајнирање и креирање
Инженеринг на карактеристики
Трансформација на необработени податоци (raw data) во карактеристики (features) кои се поинформативни, полесно разбирливи и попогодни за машинското учење.
Зошто е важно?
Дури и најмоќните алгоритми (на пример, Random Forest, XGBoost, Neural Networks) нема да дадат добри резултати ако влезните податоци (features) не содржат релевантни информации. Добро дизајнирани карактеристики можат да направат огромна разлика во точноста на моделот.
Типови на Future Engineering:
Feature Cleaning (Чистење на карактеристики)
Feature Cleaning (на македонски: чистење на карактеристики) е првиот и клучен чекор во Feature Engineering кој има за цел да ги исчисти, стандардизира и подготви карактеристиките (features) за анализа и машинско учење.
 Подобрување на квалитетот на податоците
 Отстранување на грешки и неконзистентности
 Подобрување на перформансите на моделите
 Намалување на шумот и случајноста
1. Недостасувачки вредности (NaN, null) може да ги нарушат анализите и моделите.
Методи:
df['age'].fillna(df['age'].median(), inplace=True)  # пополнување со медијана
df.dropna(subset=['column_name'], inplace=True)     # бришење редови со NaN
2. Стандардизирање на формати и типови
Некои колони можеби изгледаат правилно, но се во грешен формат:"25" (стринг) наместо број"2024-12-01" како стринг наместо datetime.
df['age'] = df['age'].astype(float)
df['date'] = pd.to_datetime(df['date’])

3. Отстранување на дупликати
df.drop_duplicates(inplace=True)
Отстранување и стандардизирање на текст
За текстуални колони: често има празни места, различни формати.
df['city'] = df['city'].str.strip().str.lower()

import pandas as pd

data = {
    'city': [' Skopje ', 'bitola', '  TETOVO', 'Skopje', 'Bitola']
}
df = pd.DataFrame(data)

df['city'] = df['city'].str.strip().str.lower()

print(df)

Отстранување и стандардизирање на текст
За текстуални колони: често има празни места, различни формати.
df['city'] = df['city'].str.strip().str.lower()

import pandas as pd

data = {
    'city': [' Skopje ', 'bitola', '  TETOVO', 'Skopje', 'Bitola']
}
df = pd.DataFrame(data)

df['city'] = df['city'].str.strip().str.lower()

print(df)

5. Работа со невалидни или неочекувани вредности
Примери:Возраст од -5
Пол: "femail" наместо "female“
df = df[(df['age'] > 0) & (df['age'] < 120)]  # филтрирање на валидни границиdf['gender'] = df['gender'].replace({'femail': 'female'})
import pandas as pd

data = {
    'name': ['Ana', 'Bojan', 'Jana', None],
    'age': ['25', 'thirty', ' 28 ', ''],
    'city': [' Skopje', 'BITOLA ', 'Tetovo', 'n/a']
}
df = pd.DataFrame(data)

# Чистење
df['age'] = pd.to_numeric(df['age'].str.strip(), errors='coerce')
df['city'] = df['city'].str.strip().str.lower().replace('n/a', pd.NA)
df.dropna(inplace=True)

print(df)

6. Нормализација на категории
Различни пишувања на иста категорија:
df['country'] = df['country'].replace({
    'USA': 'United States',
    'U.S.A.': 'United States',
    'us': 'United States'
})

Зошто е важно?
Моделите во машинско учење се многу чувствителни на "нечисти" податоци. Ако не ги исчистиме карактеристиките:
Можат да се појават грешки при обука
Може да се изгуби точност
Може да се донесат погрешни заклучоци















